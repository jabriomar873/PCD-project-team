{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-25T18:35:15.540263Z",
     "iopub.status.busy": "2025-04-25T18:35:15.539978Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Batch 0/1784 Loss: 6.6185\n",
      "Batch 100/1784 Loss: 2.9016\n",
      "Batch 200/1784 Loss: 2.8741\n",
      "Batch 300/1784 Loss: 2.8632\n",
      "Batch 400/1784 Loss: 2.8603\n",
      "Batch 500/1784 Loss: 2.8028\n",
      "Batch 600/1784 Loss: 2.3947\n",
      "Batch 700/1784 Loss: 2.2649\n",
      "Batch 800/1784 Loss: 2.0571\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"GRU-based Speech Recognition with Noise Augmentation (Corrected Reshape)\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data directory exists to avoid FileNotFoundError\n",
    "os.makedirs(\"./data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Noise Augmentation Module --------------------\n",
    "class AddGaussianNoise(nn.Module):\n",
    "    \"\"\"Adds Gaussian noise to the waveform.\"\"\"\n",
    "    def __init__(self, noise_level=0.005):\n",
    "        super().__init__()\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(waveform) * self.noise_level\n",
    "            return waveform + noise\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Audio Transforms --------------------\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    AddGaussianNoise(noise_level=0.01),\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Text Processing --------------------\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.chars = [\"'\", '<SPACE>'] + list(string.ascii_lowercase)\n",
    "        self.char_map = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.index_map = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.index_map[self.char_map['<SPACE>']] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        return [self.char_map.get(c, self.char_map['<SPACE>']) for c in text.lower()]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        return ''.join([self.index_map[i] for i in labels]).replace('<SPACE>', ' ')\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Data Processing --------------------\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    specs, labels = [], []\n",
    "    input_lengths, label_lengths = [], []\n",
    "    transform = train_audio_transforms if data_type == 'train' else valid_audio_transforms\n",
    "    for (waveform, _, utterance, *_ ) in data:\n",
    "        spec = transform(waveform).squeeze(0).transpose(0, 1)\n",
    "        specs.append(spec)\n",
    "        label = torch.tensor(text_transform.text_to_int(utterance))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0] // 2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    specs = nn.utils.rnn.pad_sequence(specs, batch_first=True)\n",
    "    specs = specs.unsqueeze(1).transpose(2, 3)  # (B,1,Feats,Time)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return specs, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Model Components --------------------\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2, 3).contiguous()\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, stride, dropout, n_feats):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_ch, out_ch, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_ch, out_ch, kernel, stride, padding=kernel//2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = CNNLayerNorm(n_feats)\n",
    "        self.ln2 = CNNLayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.ln1(x); x = nn.GELU()(x); x = self.dropout(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.ln2(x); x = nn.GELU()(x); x = self.dropout(x)\n",
    "        x = self.cnn2(x)\n",
    "        return x + residual\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x); x = nn.GELU()(x)\n",
    "        x, _ = self.gru(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Main Model --------------------\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers=3, n_rnn_layers=5, rnn_dim=512,\n",
    "                 n_class=29, n_feats=128, stride=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=1)\n",
    "        self.rescnn = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, 3, 1, dropout, n_feats//2)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        # linear expects 32*(n_feats/2) features\n",
    "        self.linear = nn.Linear(32 * (n_feats//2), rnn_dim)\n",
    "        self.rnns = nn.Sequential(*[\n",
    "            BidirectionalGRU(\n",
    "                input_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                hidden_dim=rnn_dim,\n",
    "                dropout=dropout\n",
    "            ) for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn(x)\n",
    "        batch, ch, feat_dim, seq_len = x.size()\n",
    "        # permute to (batch, seq_len, ch, feat_dim)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        # flatten channels and feat dims\n",
    "        x = x.view(batch, seq_len, ch * feat_dim)\n",
    "        x = self.linear(x)\n",
    "        x = self.rnns(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Setup --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = {\"batch_size\":16, \"epochs\":20, \"lr\":3e-4,\n",
    "          \"n_cnn_layers\":3, \"n_rnn_layers\":5, \"rnn_dim\":512,\n",
    "          \"n_class\":29, \"n_feats\":128, \"stride\":2, \"dropout\":0.2}\n",
    "\n",
    "model = SpeechRecognitionModel(**{k:params[k] for k in [\n",
    "    'n_cnn_layers','n_rnn_layers','rnn_dim','n_class','n_feats','stride','dropout']\n",
    "}).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])\n",
    "criterion = nn.CTCLoss(blank=28).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Training Utilities --------------------\n",
    "def decode(outputs):\n",
    "    \"\"\"Greedy decoder\"\"\"\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    return [text_transform.int_to_text(p.tolist()) for p in preds]\n",
    "\n",
    "def wer(ref, hyp):\n",
    "    \"\"\"Word Error Rate\"\"\"\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    return levenshtein_distance(ref_words, hyp_words) / max(len(ref_words), 1)\n",
    "\n",
    "def cer(ref, hyp):\n",
    "    \"\"\"Character Error Rate\"\"\"\n",
    "    return levenshtein_distance(ref, hyp) / max(len(ref), 1)\n",
    "\n",
    "def levenshtein_distance(a, b):\n",
    "    \"\"\"Dynamic programming implementation of Levenshtein distance\"\"\"\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    \n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            else:\n",
    "                cost = 0 if a[i-1] == b[j-1] else 1\n",
    "                dp[i][j] = min(dp[i-1][j] + 1,\n",
    "                               dp[i][j-1] + 1,\n",
    "                               dp[i-1][j-1] + cost)\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------- Training Execution --------------------\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (spectrograms, labels, input_lens, label_lens) in enumerate(loader):\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        outputs = F.log_softmax(outputs, dim=2).transpose(0, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels, input_lens, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(loader)} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = total_cer = total_wer = 0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels, input_lens, label_lens in loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            outputs = F.log_softmax(outputs, dim=2).transpose(0, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels, input_lens, label_lens)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_texts = decode(outputs.transpose(0, 1))\n",
    "            true_texts = [text_transform.int_to_text(l.tolist()) for l in labels]\n",
    "            \n",
    "            for ref, hyp in zip(true_texts, pred_texts):\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_cer = total_cer / len(loader.dataset)\n",
    "    avg_wer = total_wer / len(loader.dataset)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f} | CER: {avg_cer:.4f} | WER: {avg_wer:.4f}\")\n",
    "    return avg_loss, avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main Execution --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "        root=\"./data\",\n",
    "        url=\"train-clean-100\",\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "        root=\"./data\",\n",
    "        url=\"test-clean\",\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=pipeline_params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: data_processing(x, \"train\")\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=pipeline_params[\"batch_size\"],\n",
    "        collate_fn=lambda x: data_processing(x, \"valid\")\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    best_wer = float('inf')\n",
    "    for epoch in range(pipeline_params[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{pipeline_params['epochs']}\")\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_cer, val_wer = validate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_wer < best_wer:\n",
    "            best_wer = val_wer\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Saved new best model!\") "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
