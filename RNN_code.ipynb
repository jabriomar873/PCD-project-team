{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-25T18:51:51.237461Z",
     "iopub.status.busy": "2025-04-25T18:51:51.236650Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RNN-based Speech Recognition with Noise Augmentation (Using nn.RNN)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"RNN-based Speech Recognition with Noise Augmentation (Using nn.RNN)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data directory exists to avoid FileNotFoundError\n",
    "os.makedirs(\"./data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Noise Augmentation Module --------------------\n",
    "class AddGaussianNoise(nn.Module):\n",
    "    \"\"\"Adds Gaussian noise to the waveform.\"\"\"\n",
    "    def __init__(self, noise_level=0.005):\n",
    "        super().__init__()\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(waveform) * self.noise_level\n",
    "            return waveform + noise\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eagle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------- Audio Transforms --------------------\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    AddGaussianNoise(noise_level=0.01),\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Text Processing --------------------\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.chars = [\"'\", '<SPACE>'] + list(string.ascii_lowercase)\n",
    "        self.char_map = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.index_map = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.index_map[self.char_map['<SPACE>']] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        return [self.char_map.get(c, self.char_map['<SPACE>']) for c in text.lower()]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        return ''.join([self.index_map[i] for i in labels]).replace('<SPACE>', ' ')\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Data Processing --------------------\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    specs, labels = [], []\n",
    "    input_lengths, label_lengths = [], []\n",
    "    transform = train_audio_transforms if data_type == 'train' else valid_audio_transforms\n",
    "    for (waveform, _, utterance, *_ ) in data:\n",
    "        spec = transform(waveform).squeeze(0).transpose(0, 1)\n",
    "        specs.append(spec)\n",
    "        label = torch.tensor(text_transform.text_to_int(utterance))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0] // 2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    specs = nn.utils.rnn.pad_sequence(specs, batch_first=True)\n",
    "    specs = specs.unsqueeze(1).transpose(2, 3)  # (B,1,Feats,Time)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return specs, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Model Components --------------------\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2, 3).contiguous()\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, stride, dropout, n_feats):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_ch, out_ch, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_ch, out_ch, kernel, stride, padding=kernel//2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln1 = CNNLayerNorm(n_feats)\n",
    "        self.ln2 = CNNLayerNorm(n_feats)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.ln1(x); x = nn.GELU()(x); x = self.dropout(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.ln2(x); x = nn.GELU()(x); x = self.dropout(x)\n",
    "        x = self.cnn2(x)\n",
    "        return x + residual\n",
    "\n",
    "class BidirectionalRNN(nn.Module):\n",
    "    \"\"\"Bidirectional RNN with LayerNorm\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x); x = nn.GELU()(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main Model --------------------\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers=3, n_rnn_layers=5, rnn_dim=512,\n",
    "                 n_class=29, n_feats=128, stride=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=1)\n",
    "        self.rescnn = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, 3, 1, dropout, n_feats//2)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.linear = nn.Linear(32 * (n_feats//2), rnn_dim)\n",
    "        self.rnns = nn.Sequential(*[\n",
    "            BidirectionalRNN(\n",
    "                input_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                hidden_dim=rnn_dim,\n",
    "                dropout=dropout\n",
    "            ) for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn(x)\n",
    "        batch, ch, feat_dim, seq_len = x.size()\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = x.view(batch, seq_len, ch * feat_dim)\n",
    "        x = self.linear(x)\n",
    "        x = self.rnns(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Setup --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = {\"batch_size\":16, \"epochs\":20, \"lr\":3e-4,\n",
    "          \"n_cnn_layers\":3, \"n_rnn_layers\":5, \"rnn_dim\":512,\n",
    "          \"n_class\":29, \"n_feats\":128, \"stride\":2, \"dropout\":0.2}\n",
    "\n",
    "model = SpeechRecognitionModel(**{k:params[k] for k in [\n",
    "    'n_cnn_layers','n_rnn_layers','rnn_dim','n_class','n_feats','stride','dropout']\n",
    "}).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])\n",
    "criterion = nn.CTCLoss(blank=28).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Utilities --------------------\n",
    "def decode(outputs):\n",
    "    \"\"\"Greedy decoder\"\"\"\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    return [text_transform.int_to_text(p.tolist()) for p in preds]\n",
    "\n",
    "def wer(ref, hyp):\n",
    "    \"\"\"Word Error Rate\"\"\"\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    return levenshtein_distance(ref_words, hyp_words) / max(len(ref_words), 1)\n",
    "\n",
    "def cer(ref, hyp):\n",
    "    \"\"\"Character Error Rate\"\"\"\n",
    "    return levenshtein_distance(ref, hyp) / max(len(ref), 1)\n",
    "\n",
    "def levenshtein_distance(a, b):\n",
    "    \"\"\"Dynamic programming implementation of Levenshtein distance\"\"\"\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    \n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            else:\n",
    "                cost = 0 if a[i-1] == b[j-1] else 1\n",
    "                dp[i][j] = min(dp[i-1][j] + 1,\n",
    "                               dp[i][j-1] + 1,\n",
    "                               dp[i-1][j-1] + cost)\n",
    "    return dp[m][n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Execution --------------------\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (spectrograms, labels, input_lens, label_lens) in enumerate(loader):\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        outputs = F.log_softmax(outputs, dim=2).transpose(0, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels, input_lens, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(loader)} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = total_cer = total_wer = 0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels, input_lens, label_lens in loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            outputs = F.log_softmax(outputs, dim=2).transpose(0, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels, input_lens, label_lens)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_texts = decode(outputs.transpose(0, 1))\n",
    "            true_texts = [text_transform.int_to_text(l.tolist()) for l in labels]\n",
    "            \n",
    "            for ref, hyp in zip(true_texts, pred_texts):\n",
    "                total_cer += cer(ref, hyp)\n",
    "                total_wer += wer(ref, hyp)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_cer = total_cer / len(loader.dataset)\n",
    "    avg_wer = total_wer / len(loader.dataset)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f} | CER: {avg_cer:.4f} | WER: {avg_wer:.4f}\")\n",
    "    return avg_loss, avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23.5M/5.95G [01:10<5:04:19, 348kB/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------- Main Execution --------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIBRISPEECH\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain-clean-100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mLIBRISPEECH(\n\u001b[0;32m     11\u001b[0m         root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-clean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m         download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eagle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\datasets\\librispeech.py:113\u001b[0m, in \u001b[0;36mLIBRISPEECH.__init__\u001b[1;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m--> 113\u001b[0m         \u001b[43m_download_librispeech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please set `download=True` to download the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Eagle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\datasets\\librispeech.py:42\u001b[0m, in \u001b[0;36m_download_librispeech\u001b[1;34m(root, url)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(archive):\n\u001b[0;32m     41\u001b[0m     checksum \u001b[38;5;241m=\u001b[39m _CHECKSUMS\u001b[38;5;241m.\u001b[39mget(download_url, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m _extract_tar(archive)\n",
      "File \u001b[1;32mc:\\Users\\Eagle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:744\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[0;32m    737\u001b[0m     total\u001b[38;5;241m=\u001b[39mfile_size,\n\u001b[0;32m    738\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    741\u001b[0m     unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m    742\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m         buffer \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_DATA_CHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eagle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Eagle\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------- Main Execution --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "        root=\"./data\",\n",
    "        url=\"train-clean-100\",\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "        root=\"./data\",\n",
    "        url=\"test-clean\",\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=pipeline_params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: data_processing(x, \"train\")\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=pipeline_params[\"batch_size\"],\n",
    "        collate_fn=lambda x: data_processing(x, \"valid\")\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    best_wer = float('inf')\n",
    "    for epoch in range(pipeline_params[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{pipeline_params['epochs']}\")\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_cer, val_wer = validate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_wer < best_wer:\n",
    "            best_wer = val_wer\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Saved new best model!\") \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
