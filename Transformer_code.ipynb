{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Advanced Transformer-based Speech Recognition System (Optimized)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:30.620341Z","iopub.execute_input":"2025-04-26T11:20:30.620878Z","iopub.status.idle":"2025-04-26T11:20:30.630876Z","shell.execute_reply.started":"2025-04-26T11:20:30.620855Z","shell.execute_reply":"2025-04-26T11:20:30.630222Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'Advanced Transformer-based Speech Recognition System (Optimized)'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport math\nimport string\nimport torch.nn as nn\nimport torchaudio\nimport numpy as np\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset, Sampler\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchaudio.datasets import LIBRISPEECH\nfrom typing import Tuple, List, Dict, Optional\nfrom pathlib import Path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:30.632133Z","iopub.execute_input":"2025-04-26T11:20:30.632377Z","iopub.status.idle":"2025-04-26T11:20:35.040629Z","shell.execute_reply.started":"2025-04-26T11:20:30.632355Z","shell.execute_reply":"2025-04-26T11:20:35.040102Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.backends.cudnn.benchmark = True\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.041402Z","iopub.execute_input":"2025-04-26T11:20:35.041768Z","iopub.status.idle":"2025-04-26T11:20:35.045584Z","shell.execute_reply.started":"2025-04-26T11:20:35.041745Z","shell.execute_reply":"2025-04-26T11:20:35.044880Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# -------------------- Configuration --------------------\nclass Config:\n    # Audio parameters\n    sample_rate = 16000\n    n_mels = 64\n    win_length = 320\n    hop_length = 160\n    n_fft = 320\n    max_audio_length = 10  # Seconds\n    \n    # Vocabulary\n    vocab = list(string.ascii_lowercase) + [' ', \"'\", '<blank>']\n    \n    # Model architecture\n    cnn_channels = 32\n    num_cnn_layers = 2\n    encoder_layers = 4\n    attention_heads = 4\n    ff_dim = 1024\n    dropout = 0.1\n    emb_dim = 256\n    \n    # Training parameters\n    batch_size = 4\n    gradient_accumulation = 4\n    epochs = 30\n    lr = 3e-4\n    weight_decay = 1e-5\n    max_grad_norm = 5.0\n    warmup_steps = 1000\n    \n    # Augmentation\n    noise_snr = (15, 20)\n    time_mask = 20\n    freq_mask = 8\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.047112Z","iopub.execute_input":"2025-04-26T11:20:35.047424Z","iopub.status.idle":"2025-04-26T11:20:35.064094Z","shell.execute_reply.started":"2025-04-26T11:20:35.047409Z","shell.execute_reply":"2025-04-26T11:20:35.063419Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# -------------------- Audio Augmentations --------------------\nclass AddGaussianNoise(nn.Module):\n    def __init__(self, min_snr=5, max_snr=20):\n        super().__init__()\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n\n    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n        if self.training:\n            snr_db = torch.empty(1).uniform_(self.min_snr, self.max_snr)\n            snr_linear = 10 ** (snr_db / 20)\n            signal_power = torch.mean(waveform ** 2)\n            noise_power = signal_power / snr_linear\n            noise = torch.randn_like(waveform) * torch.sqrt(noise_power)\n            return waveform + noise\n        return waveform\n\nclass SpecAugment(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.time_mask = torchaudio.transforms.TimeMasking(config.time_mask)\n        self.freq_mask = torchaudio.transforms.FrequencyMasking(config.freq_mask)\n\n    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n        if self.training:\n            spec = self.time_mask(spec)\n            spec = self.freq_mask(spec)\n        return spec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.064791Z","iopub.execute_input":"2025-04-26T11:20:35.065119Z","iopub.status.idle":"2025-04-26T11:20:35.079211Z","shell.execute_reply.started":"2025-04-26T11:20:35.065102Z","shell.execute_reply":"2025-04-26T11:20:35.078668Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# -------------------- Audio Processing --------------------\nclass AudioProcessor:\n    @staticmethod\n    def process(waveform: torch.Tensor, train: bool = True) -> torch.Tensor:\n        max_samples = config.sample_rate * config.max_audio_length\n        if waveform.shape[-1] > max_samples:\n            waveform = waveform[..., :max_samples]\n            \n        transforms = [\n            torchaudio.transforms.MelSpectrogram(\n                sample_rate=config.sample_rate,\n                n_mels=config.n_mels,\n                n_fft=config.n_fft,\n                win_length=config.win_length,\n                hop_length=config.hop_length\n            ),\n            torchaudio.transforms.AmplitudeToDB(),\n        ]\n        \n        if train:\n            transforms = [AddGaussianNoise(*config.noise_snr)] + transforms\n            transforms.append(SpecAugment())\n            \n        return nn.Sequential(*transforms)(waveform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.079837Z","iopub.execute_input":"2025-04-26T11:20:35.080038Z","iopub.status.idle":"2025-04-26T11:20:35.096476Z","shell.execute_reply.started":"2025-04-26T11:20:35.080022Z","shell.execute_reply":"2025-04-26T11:20:35.095923Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# -------------------- Neural Modules --------------------\nclass DepthwiseConv(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),\n            nn.Conv2d(in_channels, out_channels, 1),\n            nn.GELU(),\n            nn.Dropout(config.dropout)\n        )\n        self.norm = nn.LayerNorm(out_channels)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.layers(x)\n        x = x.permute(0, 2, 3, 1)\n        x = self.norm(x)\n        return x.permute(0, 3, 1, 2)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:x.size(1)]\n        return self.dropout(x)\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(config.emb_dim, config.dropout)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=config.emb_dim,\n            nhead=config.attention_heads,\n            dim_feedforward=config.ff_dim,\n            dropout=config.dropout,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, config.encoder_layers)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        x = self.pos_encoder(x)\n        return self.encoder(x, src_key_padding_mask=mask)\n\nclass ASRTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.frontend = nn.Sequential(\n            DepthwiseConv(1, config.cnn_channels),\n            *[DepthwiseConv(config.cnn_channels, config.cnn_channels) \n              for _ in range(config.num_cnn_layers-1)]\n        )\n        self.projection = nn.Linear(config.cnn_channels * config.n_mels, config.emb_dim)\n        self.encoder = TransformerEncoder()\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(config.emb_dim),\n            nn.Linear(config.emb_dim, len(config.vocab) + 1)\n        )\n\n    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n        x = self.frontend(x)\n        x = x.permute(0, 2, 1, 3).flatten(2, 3)\n        x = self.projection(x)\n        mask = self.create_mask(lengths, x.size(1))\n        x = self.encoder(x, mask)\n        return self.classifier(x)\n\n    def create_mask(self, lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n        return (torch.arange(max_len, device=lengths.device)[None, :] >= lengths[:, None])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.097042Z","iopub.execute_input":"2025-04-26T11:20:35.097219Z","iopub.status.idle":"2025-04-26T11:20:35.110398Z","shell.execute_reply.started":"2025-04-26T11:20:35.097206Z","shell.execute_reply":"2025-04-26T11:20:35.109764Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# -------------------- Training System --------------------\nclass CTCTrainer:\n    def __init__(self):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = ASRTransformer().to(self.device)\n        self.optimizer = AdamW(self.model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        self.scaler = torch.amp.GradScaler()\n        self.criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n        \n    def train_step(self, batch: Tuple, batch_idx: int) -> float:\n        self.model.train()\n        specs, labels, spec_lens, label_lens = [t.to(self.device, non_blocking=True) for t in batch]\n        \n        with torch.autocast(device_type=self.device.type):\n            logits = self.model(specs, spec_lens)\n            log_probs = torch.log_softmax(logits, dim=-1)\n            loss = self.criterion(\n                log_probs.permute(1, 0, 2),\n                labels,\n                spec_lens,\n                label_lens\n            ) / config.gradient_accumulation\n            \n        self.scaler.scale(loss).backward()\n        \n        if (batch_idx + 1) % config.gradient_accumulation == 0:\n            self.scaler.unscale_(self.optimizer)\n            nn.utils.clip_grad_norm_(self.model.parameters(), config.max_grad_norm)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            self.optimizer.zero_grad(set_to_none=True)\n            \n            if batch_idx % (10 * config.gradient_accumulation) == 0:\n                torch.cuda.empty_cache()\n                \n        return loss.item() * config.gradient_accumulation\n\n    def validate(self, valid_loader) -> Tuple[float, float]:\n        self.model.eval()\n        total_loss, total_wer, total_samples = 0.0, 0.0, 0\n        \n        with torch.no_grad():\n            for batch in valid_loader:\n                specs, labels, spec_lens, label_lens = [t.to(self.device) for t in batch]\n                \n                with torch.autocast(device_type=self.device.type):\n                    logits = self.model(specs, spec_lens)\n                    log_probs = torch.log_softmax(logits, dim=-1)\n                    loss = self.criterion(\n                        log_probs.permute(1, 0, 2),\n                        labels,\n                        spec_lens,\n                        label_lens\n                    )\n                total_loss += loss.item() * specs.size(0)\n                \n                preds = logits.argmax(dim=-1).cpu()\n                decoded_preds = [self._decode_ctc(p) for p in preds]\n                \n                labels_cpu = labels.cpu().numpy()\n                label_lens_cpu = label_lens.cpu().numpy()\n                decoded_labels = [\n                    self._decode_ctc(label[:l]) \n                    for label, l in zip(labels_cpu, label_lens_cpu)\n                ]\n                \n                for pred, label in zip(decoded_preds, decoded_labels):\n                    total_wer += word_error_rate(label, pred)\n                    total_samples += 1\n        \n        return total_loss / len(valid_loader.dataset), total_wer / total_samples\n\n    def _decode_ctc(self, sequence: np.ndarray) -> str:\n        chars = ['<blank>'] + config.vocab\n        decoded = []\n        prev = 0\n        for idx in sequence:\n            if idx != prev:\n                if idx != 0:\n                    decoded.append(chars[idx])\n                prev = idx\n        return ''.join(decoded).strip()\n\ndef word_error_rate(ref: str, hyp: str) -> float:\n    ref_words = ref.split()\n    hyp_words = hyp.split()\n    if len(ref_words) == 0:\n        return 0.0\n    return levenshtein(ref_words, hyp_words) / len(ref_words)\n\ndef levenshtein(a: List[str], b: List[str]) -> int:\n    m, n = len(a), len(b)\n    dp = [[0]*(n+1) for _ in range(m+1)]\n    for i in range(m+1):\n        for j in range(n+1):\n            if i == 0:\n                dp[i][j] = j\n            elif j == 0:\n                dp[i][j] = i\n            else:\n                cost = 0 if a[i-1] == b[j-1] else 1\n                dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n    return dp[m][n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.111112Z","iopub.execute_input":"2025-04-26T11:20:35.111321Z","iopub.status.idle":"2025-04-26T11:20:35.132075Z","shell.execute_reply.started":"2025-04-26T11:20:35.111297Z","shell.execute_reply":"2025-04-26T11:20:35.131513Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------- Data Pipeline --------------------\nclass ASRDataset(Dataset):\n    def __init__(self, subset: str):\n        self.subset = subset\n        self.dataset = LIBRISPEECH(root=\"./data\", url=subset, download=True)\n        self.vocab = {c: i for i, c in enumerate(['<blank>'] + config.vocab)}\n        self.lengths = [self._get_trimmed_length(i) for i in range(len(self.dataset))]\n        \n    def _get_trimmed_length(self, idx: int) -> int:\n        waveform, sample_rate, _, _, n_samples, _ = self.dataset[idx]\n        return min(n_samples, config.sample_rate * config.max_audio_length)\n        \n    def __getitem__(self, idx: int) -> Tuple:\n        waveform, _, text, *_ = self.dataset[idx]\n        spec = AudioProcessor.process(waveform, train='train' in self.subset)\n        labels = torch.tensor([self.vocab.get(c, self.vocab[' ']) for c in text.lower()])\n        return spec.squeeze(0).T, labels\n    \n    def __len__(self) -> int:\n        return len(self.dataset)\n\ndef collate_fn(batch: List) -> Tuple:\n    specs, labels = zip(*batch)\n    spec_lens = torch.tensor([s.size(0) for s in specs], dtype=torch.long)\n    label_lens = torch.tensor([len(l) for l in labels], dtype=torch.long)\n    \n    specs = pad_sequence(specs, batch_first=True).unsqueeze(1)\n    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n    return specs, labels, spec_lens, label_lens\n\nclass BucketSampler(Sampler):\n    def __init__(self, lengths: List[int], batch_size: int, shuffle: bool = True):\n        self.lengths = lengths\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        indices = np.argsort(lengths).tolist()\n        self.batches = [indices[i:i+batch_size] for i in range(0, len(indices), batch_size)]\n        if shuffle:\n            np.random.shuffle(self.batches)\n    \n    def __iter__(self):\n        for batch in self.batches:\n            yield batch\n        if self.shuffle:\n            np.random.shuffle(self.batches)\n    \n    def __len__(self):\n        return len(self.batches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.133504Z","iopub.execute_input":"2025-04-26T11:20:35.134091Z","iopub.status.idle":"2025-04-26T11:20:35.150971Z","shell.execute_reply.started":"2025-04-26T11:20:35.134073Z","shell.execute_reply":"2025-04-26T11:20:35.150305Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def main():\n    # Create data directory if it doesn't exist\n    os.makedirs(\"./data\", exist_ok=True)\n    \n    try:\n        train_dataset = ASRDataset('train-clean-100')\n        valid_dataset = ASRDataset('dev-clean')\n    except Exception as e:\n        print(f\"Failed to load datasets: {e}\")\n        print(\"Please check your internet connection and disk space.\")\n        return\n\n    train_sampler = BucketSampler(train_dataset.lengths, config.batch_size)\n    valid_sampler = BucketSampler(valid_dataset.lengths, config.batch_size, shuffle=False)\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_sampler=train_sampler,\n        collate_fn=collate_fn,\n        num_workers=4,\n        pin_memory=True,\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_sampler=valid_sampler,\n        collate_fn=collate_fn,\n        num_workers=2,\n        pin_memory=True,\n    )\n    \n    trainer = CTCTrainer()\n    \n    for epoch in range(config.epochs):\n        # Training\n        trainer.model.train()\n        total_loss = 0.0\n        for batch_idx, batch in enumerate(train_loader):\n            loss = trainer.train_step(batch, batch_idx)\n            total_loss += loss\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Epoch {epoch} Batch {batch_idx} Loss: {loss:.3f}\")\n        \n        # Validation\n        val_loss, val_wer = trainer.validate(valid_loader)\n        print(f\"Epoch {epoch} | Train Loss: {total_loss/len(train_loader):.3f} | Val Loss: {val_loss:.3f} | WER: {val_wer:.3f}\")\n        \n        # Save checkpoint\n        torch.save(trainer.model.state_dict(), f\"asr_epoch_{epoch}.pt\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T11:20:35.151702Z","iopub.execute_input":"2025-04-26T11:20:35.151919Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5.95G/5.95G [03:12<00:00, 33.2MB/s] \n100%|██████████| 322M/322M [00:10<00:00, 33.4MB/s] \n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 Batch 9 Loss: 3.506\nEpoch 0 Batch 19 Loss: 3.435\nEpoch 0 Batch 29 Loss: 2.942\nEpoch 0 Batch 39 Loss: 2.912\nEpoch 0 Batch 49 Loss: 2.972\nEpoch 0 Batch 59 Loss: 2.951\nEpoch 0 Batch 69 Loss: 2.995\nEpoch 0 Batch 79 Loss: 2.917\nEpoch 0 Batch 89 Loss: 2.914\nEpoch 0 Batch 99 Loss: 2.948\nEpoch 0 Batch 109 Loss: 2.895\nEpoch 0 Batch 119 Loss: 2.912\nEpoch 0 Batch 129 Loss: 2.966\nEpoch 0 Batch 139 Loss: 2.842\nEpoch 0 Batch 149 Loss: 2.900\nEpoch 0 Batch 159 Loss: 2.885\nEpoch 0 Batch 169 Loss: 2.841\nEpoch 0 Batch 179 Loss: 2.977\nEpoch 0 Batch 189 Loss: 2.900\nEpoch 0 Batch 199 Loss: 2.890\nEpoch 0 Batch 209 Loss: 2.902\nEpoch 0 Batch 219 Loss: 2.925\nEpoch 0 Batch 229 Loss: 2.967\nEpoch 0 Batch 239 Loss: 2.949\nEpoch 0 Batch 249 Loss: 3.068\nEpoch 0 Batch 259 Loss: 2.962\nEpoch 0 Batch 269 Loss: 2.880\nEpoch 0 Batch 279 Loss: 2.893\nEpoch 0 Batch 289 Loss: 2.911\nEpoch 0 Batch 299 Loss: 2.862\nEpoch 0 Batch 309 Loss: 2.893\nEpoch 0 Batch 319 Loss: 2.870\nEpoch 0 Batch 329 Loss: 2.861\nEpoch 0 Batch 339 Loss: 2.887\nEpoch 0 Batch 349 Loss: 2.884\nEpoch 0 Batch 359 Loss: 2.838\nEpoch 0 Batch 369 Loss: 2.839\nEpoch 0 Batch 379 Loss: 2.886\nEpoch 0 Batch 389 Loss: 2.873\nEpoch 0 Batch 399 Loss: 2.868\nEpoch 0 Batch 409 Loss: 2.900\nEpoch 0 Batch 419 Loss: 2.955\nEpoch 0 Batch 429 Loss: 2.893\nEpoch 0 Batch 439 Loss: 2.836\nEpoch 0 Batch 449 Loss: 2.793\nEpoch 0 Batch 459 Loss: 2.867\nEpoch 0 Batch 469 Loss: 2.887\nEpoch 0 Batch 479 Loss: 2.928\nEpoch 0 Batch 489 Loss: 2.841\nEpoch 0 Batch 499 Loss: 2.926\nEpoch 0 Batch 509 Loss: 2.890\nEpoch 0 Batch 519 Loss: 2.867\nEpoch 0 Batch 529 Loss: 2.849\nEpoch 0 Batch 539 Loss: 3.025\nEpoch 0 Batch 549 Loss: 2.963\nEpoch 0 Batch 559 Loss: 2.926\nEpoch 0 Batch 569 Loss: 2.893\nEpoch 0 Batch 579 Loss: 2.903\nEpoch 0 Batch 589 Loss: 2.867\nEpoch 0 Batch 599 Loss: 2.908\nEpoch 0 Batch 609 Loss: 2.905\nEpoch 0 Batch 619 Loss: 2.992\nEpoch 0 Batch 629 Loss: 2.911\nEpoch 0 Batch 639 Loss: 2.890\nEpoch 0 Batch 649 Loss: 2.939\nEpoch 0 Batch 659 Loss: 2.974\nEpoch 0 Batch 669 Loss: 2.831\nEpoch 0 Batch 679 Loss: 2.839\nEpoch 0 Batch 689 Loss: 2.895\nEpoch 0 Batch 699 Loss: 2.914\nEpoch 0 Batch 709 Loss: 2.964\nEpoch 0 Batch 719 Loss: 2.926\nEpoch 0 Batch 729 Loss: 2.889\nEpoch 0 Batch 739 Loss: 2.883\nEpoch 0 Batch 749 Loss: 2.881\nEpoch 0 Batch 759 Loss: 2.920\nEpoch 0 Batch 769 Loss: 2.922\nEpoch 0 Batch 779 Loss: 2.963\nEpoch 0 Batch 789 Loss: 2.890\nEpoch 0 Batch 799 Loss: 2.879\nEpoch 0 Batch 809 Loss: 2.865\nEpoch 0 Batch 819 Loss: 2.884\nEpoch 0 Batch 829 Loss: 2.888\nEpoch 0 Batch 839 Loss: 2.937\nEpoch 0 Batch 849 Loss: 2.845\nEpoch 0 Batch 859 Loss: 2.897\nEpoch 0 Batch 869 Loss: 2.964\nEpoch 0 Batch 879 Loss: 2.879\nEpoch 0 Batch 889 Loss: 2.849\nEpoch 0 Batch 899 Loss: 2.919\nEpoch 0 Batch 909 Loss: 2.917\nEpoch 0 Batch 919 Loss: 2.796\nEpoch 0 Batch 929 Loss: 2.871\nEpoch 0 Batch 939 Loss: 2.845\nEpoch 0 Batch 949 Loss: 2.908\nEpoch 0 Batch 959 Loss: 2.963\nEpoch 0 Batch 969 Loss: 2.946\nEpoch 0 Batch 979 Loss: 2.894\nEpoch 0 Batch 989 Loss: 2.865\nEpoch 0 Batch 999 Loss: 3.003\nEpoch 0 Batch 1009 Loss: 2.884\nEpoch 0 Batch 1019 Loss: 2.874\nEpoch 0 Batch 1029 Loss: 2.896\nEpoch 0 Batch 1039 Loss: 2.965\nEpoch 0 Batch 1049 Loss: 2.898\nEpoch 0 Batch 1059 Loss: 2.860\nEpoch 0 Batch 1069 Loss: 2.890\nEpoch 0 Batch 1079 Loss: 2.885\nEpoch 0 Batch 1089 Loss: 2.887\nEpoch 0 Batch 1099 Loss: 2.874\nEpoch 0 Batch 1109 Loss: 2.892\nEpoch 0 Batch 1119 Loss: 2.875\nEpoch 0 Batch 1129 Loss: 2.846\nEpoch 0 Batch 1139 Loss: 2.878\nEpoch 0 Batch 1149 Loss: 2.906\nEpoch 0 Batch 1159 Loss: 3.005\nEpoch 0 Batch 1169 Loss: 2.916\nEpoch 0 Batch 1179 Loss: 2.906\nEpoch 0 Batch 1189 Loss: 2.875\nEpoch 0 Batch 1199 Loss: 2.869\nEpoch 0 Batch 1209 Loss: 2.902\nEpoch 0 Batch 1219 Loss: 2.911\nEpoch 0 Batch 1229 Loss: 2.895\nEpoch 0 Batch 1239 Loss: 2.883\nEpoch 0 Batch 1249 Loss: 2.865\nEpoch 0 Batch 1259 Loss: 2.963\nEpoch 0 Batch 1269 Loss: 2.842\nEpoch 0 Batch 1279 Loss: 2.897\nEpoch 0 Batch 1289 Loss: 2.848\nEpoch 0 Batch 1299 Loss: 2.837\nEpoch 0 Batch 1309 Loss: 2.940\nEpoch 0 Batch 1319 Loss: 2.905\nEpoch 0 Batch 1329 Loss: 2.900\nEpoch 0 Batch 1339 Loss: 2.864\nEpoch 0 Batch 1349 Loss: 2.835\nEpoch 0 Batch 1359 Loss: 2.919\nEpoch 0 Batch 1369 Loss: 2.865\nEpoch 0 Batch 1379 Loss: 2.980\nEpoch 0 Batch 1389 Loss: 2.880\nEpoch 0 Batch 1399 Loss: 2.921\nEpoch 0 Batch 1409 Loss: 2.836\nEpoch 0 Batch 1419 Loss: 2.837\nEpoch 0 Batch 1429 Loss: 2.831\nEpoch 0 Batch 1439 Loss: 2.866\nEpoch 0 Batch 1449 Loss: 2.913\nEpoch 0 Batch 1459 Loss: 2.888\nEpoch 0 Batch 1469 Loss: 2.827\nEpoch 0 Batch 1479 Loss: 2.897\nEpoch 0 Batch 1489 Loss: 2.924\nEpoch 0 Batch 1499 Loss: 2.895\nEpoch 0 Batch 1509 Loss: 2.816\nEpoch 0 Batch 1519 Loss: 2.823\nEpoch 0 Batch 1529 Loss: 2.866\nEpoch 0 Batch 1539 Loss: 2.865\nEpoch 0 Batch 1549 Loss: 2.900\nEpoch 0 Batch 1559 Loss: 2.864\n","output_type":"stream"}],"execution_count":null}]}